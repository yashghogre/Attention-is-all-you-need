{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T12:15:45.012623Z",
     "iopub.status.busy": "2025-02-24T12:15:45.012291Z",
     "iopub.status.idle": "2025-02-24T12:15:49.503398Z",
     "shell.execute_reply": "2025-02-24T12:15:49.502380Z",
     "shell.execute_reply.started": "2025-02-24T12:15:45.012594Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T12:15:49.505001Z",
     "iopub.status.busy": "2025-02-24T12:15:49.504699Z",
     "iopub.status.idle": "2025-02-24T12:15:56.128576Z",
     "shell.execute_reply": "2025-02-24T12:15:56.127706Z",
     "shell.execute_reply.started": "2025-02-24T12:15:49.504977Z"
    },
    "id": "f6e2f818-e812-4ab2-98a1-013a4c6c7b62",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Importing Libraries\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torcheval.metrics.functional import bleu_score\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T12:15:56.130464Z",
     "iopub.status.busy": "2025-02-24T12:15:56.130084Z",
     "iopub.status.idle": "2025-02-24T12:15:56.209289Z",
     "shell.execute_reply": "2025-02-24T12:15:56.208563Z",
     "shell.execute_reply.started": "2025-02-24T12:15:56.130442Z"
    },
    "id": "bbefa233-9d67-4ba2-b8d0-8ac81687e2ab",
    "outputId": "a0ef26c1-cb33-4737-9e0d-5a35b5725c06",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Model Hyper-parameters\n",
    "'''\n",
    "n_emb = 256\n",
    "vocab_size = 1000\n",
    "seq_len = 64\n",
    "batch_size = 64\n",
    "num_heads = 4\n",
    "n_dropout = 0.1\n",
    "torch.manual_seed(1111)\n",
    "ffwd_w = 1024\n",
    "num_sa_blocks = 4\n",
    "num_ca_blocks = 4\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T12:15:56.210625Z",
     "iopub.status.busy": "2025-02-24T12:15:56.210415Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Tokenization\n",
    "'''\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "\n",
    "trainer = trainers.BpeTrainer(\n",
    "        vocab_size = vocab_size,\n",
    "        special_tokens = [\"[START]\", \"[END]\", \"[PAD]\"]\n",
    "        )\n",
    "\n",
    "tokenizer.train(files = [\"/kaggle/input/trans-data/source_train.txt\", \"/kaggle/input/trans-data/target_train.txt\"], trainer=trainer)\n",
    "\n",
    "tokenizer.save(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7df02d40-a675-4b56-ac60-45f2e559948c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Opening Files and Embedding Vector\n",
    "'''\n",
    "\n",
    "with open('/kaggle/input/trans-data/source_train.txt') as f:\n",
    "    trn_eng = f.readlines()\n",
    "\n",
    "trn_eng = [l[:-1] for l in trn_eng]\n",
    "trn_eng = trn_eng[:500000]\n",
    "\n",
    "with open('/kaggle/input/trans-data/target_train.txt') as f:\n",
    "    trn_hin = f.readlines()\n",
    "\n",
    "trn_hin = [l[:-1] for l in trn_hin]\n",
    "trn_hin = trn_hin[:500000]\n",
    "\n",
    "with open('/kaggle/input/trans-data/source_test.txt') as f:\n",
    "    tst_eng = f.readlines()\n",
    "\n",
    "tst_eng = [l[:-1] for l in tst_eng]\n",
    "\n",
    "with open('/kaggle/input/trans-data/target_test.txt') as f:\n",
    "    tst_hin = f.readlines()\n",
    "\n",
    "tst_hin = [l[:-1] for l in tst_hin]\n",
    "\n",
    "with open('/kaggle/input/trans-data/source_valid.txt') as f:\n",
    "    val_eng = f.readlines()\n",
    "\n",
    "val_eng = [l[:-1] for l in val_eng]\n",
    "\n",
    "with open('/kaggle/input/trans-data/target_valid.txt') as f:\n",
    "    val_hin = f.readlines()\n",
    "\n",
    "val_hin = [l[:-1] for l in val_hin]\n",
    "\n",
    "embeds = nn.Embedding(vocab_size, n_emb, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(trn_eng), len(trn_hin), len(tst_eng), len(tst_hin), len(val_eng), len(val_hin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1005af52-1e5c-4121-bdb6-5d0a8478cdaf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_tensor(token_id, tokenizer, embeds, pos_encoder, device):\n",
    "    token_id = torch.tensor(token_id, device=device)\n",
    "    emb_token = embeds(token_id).to(device)\n",
    "    # Reshape for positional encoding (add batch and sequence dimensions)\n",
    "    emb_token = emb_token.view(1, 1, -1)\n",
    "    # Apply positional encoding\n",
    "    emb_token = pos_encoder(emb_token, seq_len=1)\n",
    "    return emb_token\n",
    "\n",
    "def get_tensor_for_inf_x(x, tokenizer, embeds, pos_encoder, seq_len, n_emb, device):\n",
    "    tokens = tokenizer.encode(x).tokens\n",
    "    tokens.insert(0, \"[START]\")\n",
    "    tokens.append(\"[END]\")\n",
    "\n",
    "    if len(tokens) > seq_len:\n",
    "        return \"Please make sure the input is not too long\"\n",
    "\n",
    "    while seq_len - len(tokens) > 0:\n",
    "        tokens.append(\"[PAD]\")\n",
    "\n",
    "    tokens = [tokenizer.token_to_id(i) for i in tokens]\n",
    "    t_tokens = torch.tensor(tokens, dtype=torch.long, device=device)\n",
    "    t_t_embs = embeds(t_tokens).to(device)\n",
    "    \n",
    "    # Use the positional encoder\n",
    "    t_t_embs = t_t_embs.view(1, seq_len, n_emb)\n",
    "    t_t_embs = pos_encoder(t_t_embs)\n",
    "\n",
    "    return t_t_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d42f9b5f-4b42-440b-8a7f-52663091f712",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Prepare Batch\n",
    "'''\n",
    "def get_batch(transformer, split=\"train\", batch_size=16, seq_len=256):\n",
    "    inp_data = trn_eng if split == \"train\" else tst_eng\n",
    "    out_data = trn_hin if split == \"train\" else tst_hin\n",
    "    \n",
    "    # Initialize tensors\n",
    "    inp_btc = torch.zeros(batch_size, seq_len, transformer.n_emb, device=transformer.device)\n",
    "    out_btc = torch.zeros(batch_size, seq_len, transformer.n_emb, device=transformer.device)\n",
    "    target = torch.empty(batch_size, seq_len, dtype=torch.long, device=transformer.device)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Get random sample that fits within sequence length\n",
    "        while True:\n",
    "            btc_num = random.randint(0, len(inp_data)-1)\n",
    "            inp = inp_data[btc_num]\n",
    "            out = out_data[btc_num]\n",
    "\n",
    "            inp_tokens = transformer.tokenizer.encode(inp).tokens\n",
    "            out_tokens = transformer.tokenizer.encode(out).tokens\n",
    "\n",
    "            inp_tokens.insert(0, \"[START]\")\n",
    "            inp_tokens.append(\"[END]\")\n",
    "            out_tokens.insert(0, \"[START]\")\n",
    "            out_tokens.append(\"[END]\")\n",
    "\n",
    "            if len(inp_tokens) <= seq_len and len(out_tokens) <= seq_len:\n",
    "                break\n",
    "\n",
    "        # Pad sequences\n",
    "        while len(inp_tokens) < seq_len:\n",
    "            inp_tokens.append(\"[PAD]\")\n",
    "        while len(out_tokens) < seq_len:\n",
    "            out_tokens.append(\"[PAD]\")\n",
    "\n",
    "        # Prepare decoder input and target\n",
    "        dec_inp = out_tokens[:-1]\n",
    "        target_tokens = out_tokens[1:]\n",
    "\n",
    "        while len(dec_inp) < seq_len:\n",
    "            dec_inp.append(\"[PAD]\")\n",
    "        while len(target_tokens) < seq_len:\n",
    "            target_tokens.append(\"[PAD]\")\n",
    "\n",
    "        # Convert tokens to ids\n",
    "        inp_ids = [transformer.tokenizer.token_to_id(t) for t in inp_tokens]\n",
    "        out_ids = [transformer.tokenizer.token_to_id(t) for t in dec_inp]\n",
    "        target_ids = [transformer.tokenizer.token_to_id(t) for t in target_tokens]\n",
    "\n",
    "        # Convert to tensors\n",
    "        t_inp = torch.tensor(inp_ids, dtype=torch.long, device=transformer.device)\n",
    "        t_out = torch.tensor(out_ids, dtype=torch.long, device=transformer.device)\n",
    "        t_target = torch.tensor(target_ids, dtype=torch.long, device=transformer.device)\n",
    "\n",
    "        # Store target\n",
    "        target[i] = t_target\n",
    "\n",
    "        # Get embeddings\n",
    "        t_inp_emb = embeds(t_inp)\n",
    "        t_out_emb = embeds(t_out)\n",
    "\n",
    "        # Apply positional encoding\n",
    "        t_inp_emb = t_inp_emb.unsqueeze(0)  # Add batch dimension\n",
    "        t_out_emb = t_out_emb.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        inp_emb = transformer.pos_encoder(t_inp_emb).squeeze(0)\n",
    "        out_emb = transformer.pos_encoder(t_out_emb).squeeze(0)\n",
    "\n",
    "        # Store in batch tensors\n",
    "        inp_btc[i] = inp_emb\n",
    "        out_btc[i] = out_emb\n",
    "\n",
    "    return inp_btc, out_btc, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for sen in trn_eng:\n",
    "    inp_tokens = tokenizer.encode(sen).tokens\n",
    "    inp_tokens.insert(0, \"[START]\")\n",
    "    inp_tokens.append(\"[END]\")\n",
    "\n",
    "    if len(inp_tokens) <= 64:\n",
    "        count += 1\n",
    "\n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for sen in trn_hin:\n",
    "    inp_tokens = tokenizer.encode(sen).tokens\n",
    "    inp_tokens.insert(0, \"[START]\")\n",
    "    inp_tokens.append(\"[END]\")\n",
    "\n",
    "    if len(inp_tokens) <= 64:\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43baa4d1-4f4f-4f91-a6e9-3406b462ca8e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Positional Embedding\n",
    "'''\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=512, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.device = device\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model, device=device)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float, device=device).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, device=device).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x, seq_len=None):\n",
    "        if seq_len is None:\n",
    "            seq_len = x.size(1)\n",
    "        return x + self.pe[:seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5c91d67-75e3-4841-a35c-28e77bd11287",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Encoder\n",
    "'''\n",
    "\n",
    "class SA_Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.wk = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
    "        self.wq = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
    "        self.wv = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
    "\n",
    "    def forward(self, ini_emb):\n",
    "        k = self.wk(ini_emb)\n",
    "        q = self.wq(ini_emb)\n",
    "        v = self.wv(ini_emb)\n",
    "\n",
    "        k_mul = q @ k.transpose(-2, -1)\n",
    "        scaling = k_mul * (n_emb**-0.5)\n",
    "        sm_mul = F.softmax(scaling, dim=-1)\n",
    "        v_mul = sm_mul @ v\n",
    "\n",
    "        return v_mul\n",
    "\n",
    "class SA_MultiHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_size = n_emb // num_heads\n",
    "        self.heads = nn.ModuleList([SA_Head(self.head_size) for _ in range(num_heads)])\n",
    "        self.lyr = nn.Linear(n_emb, n_emb, bias=False, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.lyr(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SA_Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mh_atn = SA_MultiHead()\n",
    "        self.dropout = nn.Dropout(n_dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_emb, device=device)\n",
    "        self.ln2 = nn.LayerNorm(n_emb, device=device)\n",
    "        self.ffwd = nn.Sequential(\n",
    "                nn.Linear(n_emb, ffwd_w, device=device),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(ffwd_w, n_emb, device=device)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.ln1(x)\n",
    "        out = self.mh_atn(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out + x\n",
    "        out1 = self.ln2(out)\n",
    "        out1 = self.ffwd(out)\n",
    "        out1 = out1 + out\n",
    "\n",
    "        return out1\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sa_blocks_list = []\n",
    "\n",
    "        for _ in range(num_sa_blocks):\n",
    "            self.sa_blocks_list.append(SA_Block())\n",
    "\n",
    "        self.sa_blocks = nn.Sequential(*self.sa_blocks_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.sa_blocks(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1d32b3aa-e639-4b7a-8384-c9bd25c9a848",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Decoder\n",
    "'''\n",
    "\n",
    "class MA_Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.kw = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
    "        self.qw = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
    "        self.vw = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        k = self.kw(x)\n",
    "        q = self.qw(x)\n",
    "        v = self.vw(x)\n",
    "        mask = torch.triu(torch.full((x.size(1), x.size(1)), float('-inf'), device=device), diagonal=1)\n",
    "        out = q @ k.transpose(-2, -1)\n",
    "        out = out * (n_emb ** -0.5)\n",
    "        out = out + mask\n",
    "        out = F.softmax(out, dim=-1)\n",
    "        out = out @ v\n",
    "\n",
    "        return out\n",
    "\n",
    "class MA_MultiHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_size = n_emb // num_heads\n",
    "        self.heads = nn.ModuleList([MA_Head(self.head_size) for _ in range(num_heads)])\n",
    "        self.lyr = nn.Linear(n_emb, n_emb, bias=False, device=device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.lyr(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class CA_Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.kw = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
    "        self.qw = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
    "        self.vw = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        q = self.qw(x_dec)\n",
    "        k = self.kw(x_enc)\n",
    "        v = self.vw(x_enc)\n",
    "\n",
    "        out = q @ k.transpose(-2, -1)\n",
    "        out = out * (n_emb ** -0.5)\n",
    "        out = F.softmax(out, dim=-1)\n",
    "        out = out @ v\n",
    "\n",
    "        return out\n",
    "\n",
    "class CA_MultiHead(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_size = n_emb // num_heads\n",
    "        self.heads = nn.ModuleList([CA_Head(self.head_size) for _ in range(num_heads)])\n",
    "        self.lyr = nn.Linear(n_emb, n_emb, bias=False, device=device)\n",
    "\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        out = torch.cat([h(x_enc, x_dec) for h in self.heads], dim=-1)\n",
    "        out = self.lyr(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class CA_Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mmh_attn = MA_MultiHead()\n",
    "        self.do_1 = nn.Dropout(n_dropout)\n",
    "        self.cah_attn = CA_MultiHead()\n",
    "        self.do_2 = nn.Dropout(n_dropout)\n",
    "        self.ffwd = nn.Sequential(\n",
    "                nn.Linear(n_emb, ffwd_w, device=device),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(ffwd_w, n_emb, device=device)\n",
    "                )\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(n_emb, device=device)\n",
    "        self.ln2 = nn.LayerNorm(n_emb, device=device)\n",
    "        self.ln3 = nn.LayerNorm(n_emb, device=device)\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        out = self.ln1(x_dec)\n",
    "        out = self.mmh_attn(out)\n",
    "        out = self.do_1(out)\n",
    "        out = out + x_dec\n",
    "        out1 = self.ln2(out)\n",
    "        out1 = self.cah_attn(x_enc, out1)\n",
    "        out1 = self.do_2(out1)\n",
    "        out1 = out1 + out\n",
    "        out2 = self.ffwd(out1)\n",
    "        out2 = out2 + out1\n",
    "        out2 = self.ln3(out2)\n",
    "\n",
    "        return out2\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ca_blocks_list = []\n",
    "\n",
    "        for _ in range(num_ca_blocks):\n",
    "            self.ca_blocks_list.append(CA_Block())\n",
    "\n",
    "        self.ca_blocks = nn.ModuleList(self.ca_blocks_list)\n",
    "\n",
    "        self.lyr = nn.Linear(n_emb, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, x_enc, x_dec, split=\"training\"):\n",
    "        # if purp == \"training\":\n",
    "        for block in self.ca_blocks:\n",
    "            x_dec = block(x_enc, x_dec)\n",
    "\n",
    "        '''\n",
    "        Final Layer\n",
    "        '''\n",
    "        if split == \"training\":\n",
    "          logits = self.lyr(x_dec)\n",
    "          out = F.softmax(logits, dim=-1)\n",
    "          out = torch.argmax(out, dim=-1)\n",
    "\n",
    "        if split == \"inference\":\n",
    "          logits = self.lyr(x_dec[:, -1, :])\n",
    "          out = F.softmax(logits, dim=-1)\n",
    "          out = torch.argmax(out, -1)\n",
    "\n",
    "        return logits, out\n",
    "\n",
    "        # else if purp == \"inference\":\n",
    "\n",
    "\n",
    "        # return x_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75319c99-4115-47fb-abb6-3a0f7792e3ea",
    "outputId": "1e06d21e-7bcd-41d4-d5d1-9be384616faf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Last Layer\n",
    "'''\n",
    "'''\n",
    "class Final_Layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lyr = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.lyr(x)\n",
    "        out = F.softmax(logits, dim=-1)\n",
    "        out = torch.max(out, -1)\n",
    "\n",
    "        return logits, out\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a5285da-bd9a-4fcb-9fb7-0da8faad333e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Combined Transformer Class\n",
    "'''\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, n_emb, seq_len, device, tokenizer):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(n_emb, seq_len, device)\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        # self.embeds = nn.Embedding(vocab_size, n_emb, device=device)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.n_emb = n_emb\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, x_enc, x_dec, targets=None):\n",
    "        res_enc = self.encoder(x_enc)\n",
    "        logits, out = self.decoder(res_enc, x_dec)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets, ignore_index=2)  # Assuming 2 is the PAD token ID\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def get_batch(self, split=\"train\", batch_size=16):\n",
    "        \"\"\"Convenience method to get a batch\"\"\"\n",
    "        return get_batch(self, split, batch_size, self.seq_len)\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Initialize with start token\n",
    "        x_dec = get_tensor(\n",
    "            tokenizer.token_to_id(\"[START]\"), \n",
    "            self.tokenizer, \n",
    "            embeds, \n",
    "            self.pos_encoder, \n",
    "            self.device\n",
    "        )\n",
    "        \n",
    "        # Get input embeddings\n",
    "        inp_embs = get_tensor_for_inf_x(\n",
    "            x, \n",
    "            self.tokenizer, \n",
    "            embeds, \n",
    "            self.pos_encoder, \n",
    "            self.seq_len, \n",
    "            self.n_emb, \n",
    "            self.device\n",
    "        )\n",
    "        \n",
    "        res_enc = self.encoder(inp_embs)\n",
    "        final_sentence = []\n",
    "        max_len = 50\n",
    "        \n",
    "        while len(final_sentence) <= max_len:\n",
    "            logits, out = self.decoder(res_enc, x_dec, split=\"inference\")\n",
    "            final_token = self.tokenizer.id_to_token(out[-1])\n",
    "            \n",
    "            if final_token == \"[END]\":\n",
    "                break\n",
    "                \n",
    "            final_sentence.append(final_token)\n",
    "            \n",
    "            # Get new token embedding with positional encoding\n",
    "            new_embed = get_tensor(\n",
    "                out[-1],\n",
    "                self.tokenizer,\n",
    "                embeds,\n",
    "                self.pos_encoder,\n",
    "                self.device\n",
    "            )\n",
    "            \n",
    "            x_dec = torch.cat([x_dec, new_embed], dim=1)  # Concatenate along sequence dimension\n",
    "\n",
    "        return \" \".join(final_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_step(transformer, optimizer):\n",
    "    # Get batch\n",
    "    inp_btc, out_btc, targets = transformer.get_batch(\"train\")\n",
    "    \n",
    "    # Forward pass\n",
    "    logits, loss = transformer(inp_btc, out_btc, targets)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "# Training loop example\n",
    "def train_transformer(transformer, lr, epochs=10):\n",
    "    optimizer = torch.optim.AdamW(transformer.parameters(), lr=lr, betas=(0.9, 0.98), weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1000, T_mult=1, eta_min=1e-6)\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        transformer.train()\n",
    "        train_losses = []\n",
    "\n",
    "        num_steps = len(trn_eng) // batch_size\n",
    "        eval_interval = num_steps // 10\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            # loss = train_step(transformer, optimizer)\n",
    "            inp_btc, out_btc, targets = transformer.get_batch(\"train\")\n",
    "        \n",
    "            # Forward pass\n",
    "            logits, loss = transformer(inp_btc, out_btc, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(transformer.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            if step % eval_interval == 0:\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                print(f\"Epoch: {epoch+1} \\t | step: {step} \\t | lr: {current_lr} \\t | loss: {loss:.4f}\")\n",
    "\n",
    "        transformer.eval()\n",
    "        test_losses = []\n",
    "        for _ in range(100):\n",
    "            with torch.no_grad():\n",
    "                inp_btc, out_btc, targets = transformer.get_batch(\"test\")\n",
    "                _, eval_loss = transformer(inp_btc, out_btc, targets)\n",
    "                test_losses.append(eval_loss.item())\n",
    "\n",
    "        avg_trn_loss = sum(train_losses) / len(train_losses)  \n",
    "        avg_tst_loss = sum(test_losses) / len(test_losses)\n",
    "\n",
    "        print(\"--------------------------------------\")\n",
    "        print(f\"Epoch {epoch+1} summary\")\n",
    "        print(f\"Avg Train loss: {avg_trn_loss}\")\n",
    "        print(f\"Avg Test loss: {avg_tst_loss}\")\n",
    "        print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize transformer\n",
    "transformer = Transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    n_emb=n_emb,\n",
    "    seq_len=seq_len,\n",
    "    device=device,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trans_params = sum(p.numel() for p in transformer.parameters())\n",
    "print(f\"\\n Total Parameters = {trans_params}\\n\")\n",
    "\n",
    "# Train the model\n",
    "train_transformer(transformer, 3e-4, 2)\n",
    "\n",
    "# Or get a single batch manually\n",
    "# inp_batch, out_batch, targets = transformer.get_batch(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e1409d3-99ad-41af-8a42-cb8016b85bc3",
    "outputId": "84c63c03-b5b9-4384-c669-743a2d950001",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num = 244\n",
    "print(val_eng[num])\n",
    "print(val_hin[num])\n",
    "print(transformer.predict(\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ref_hin = [[sen] for sen in val_hin]\n",
    "# can_hin = [transformer.predict(val_eng_sen) for val_eng_sen in val_eng]\n",
    "\n",
    "# b_score = bleu_score(can_hin, ref_hin, n_gram=4)\n",
    "# b_score\n",
    "num = 100\n",
    "bleu_score([transformer.predict(val_eng[num])], [[val_hin[num]]], n_gram=4)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6698832,
     "sourceId": 10807609,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
